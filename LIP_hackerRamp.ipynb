{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LIP_hackerRamp.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhavi-Khator/Myntra_Hackerramp-21/blob/bhavi/LIP_hackerRamp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CFWEReRPTRb"
      },
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import threading\n",
        "from torchvision.models.densenet import densenet121, densenet161\n",
        "import os\n",
        "import argparse\n",
        "import logging\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import time\n",
        "import pandas as pd\n",
        "import urllib.request\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzCUAVZE3Q12"
      },
      "source": [
        "class _DenseLayer(nn.Sequential):\n",
        "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
        "        super(_DenseLayer, self).__init__()\n",
        "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
        "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
        "                                            growth_rate, kernel_size=1, stride=1, bias=False)),\n",
        "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
        "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
        "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
        "                                            kernel_size=3, stride=1, padding=1, bias=False)),\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        new_features = super(_DenseLayer, self).forward(x)\n",
        "        if self.drop_rate > 0:\n",
        "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
        "        return torch.cat([x, new_features], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaWf-U203i22"
      },
      "source": [
        "class _DenseBlock(nn.Sequential):\n",
        "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
        "        super(_DenseBlock, self).__init__()\n",
        "        for i in range(num_layers):\n",
        "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
        "            self.add_module('denselayer%d' % (i + 1), layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp9prFxj3mOb"
      },
      "source": [
        "class _Transition(nn.Sequential):\n",
        "    def __init__(self, num_input_features, num_output_features, downsample=True):\n",
        "        super(_Transition, self).__init__()\n",
        "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
        "        self.add_module('relu', nn.ReLU(inplace=True))\n",
        "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
        "                                          kernel_size=1, stride=1, bias=False))\n",
        "        if downsample:\n",
        "            self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
        "        else:\n",
        "            self.add_module('pool', nn.AvgPool2d(kernel_size=1, stride=1))  # compatibility hack"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnPlYii83qQU"
      },
      "source": [
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
        "                 num_init_features=64, bn_size=4, drop_rate=0, pretrained=True):\n",
        "\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        # First convolution\n",
        "        self.start_features = nn.Sequential(OrderedDict([\n",
        "            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n",
        "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
        "            ('relu0', nn.ReLU(inplace=True)),\n",
        "            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n",
        "        ]))\n",
        "\n",
        "        # Each denseblock\n",
        "        num_features = num_init_features\n",
        "\n",
        "        init_weights = list(densenet121(pretrained=True).features.children())\n",
        "        start = 0\n",
        "        for i, c in enumerate(self.start_features.children()):\n",
        "            if pretrained:\n",
        "                c.load_state_dict(init_weights[i].state_dict())\n",
        "            start += 1\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for i, num_layers in enumerate(block_config):\n",
        "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
        "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
        "            if pretrained:\n",
        "                block.load_state_dict(init_weights[start].state_dict())\n",
        "            start += 1\n",
        "            self.blocks.append(block)\n",
        "            setattr(self, 'denseblock%d' % (i + 1), block)\n",
        "\n",
        "            num_features = num_features + num_layers * growth_rate\n",
        "            if i != len(block_config) - 1:\n",
        "                downsample = i < 1\n",
        "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2,\n",
        "                                    downsample=downsample)\n",
        "                if pretrained:\n",
        "                    trans.load_state_dict(init_weights[start].state_dict())\n",
        "                start += 1\n",
        "                self.blocks.append(trans)\n",
        "                setattr(self, 'transition%d' % (i + 1), trans)\n",
        "                num_features = num_features // 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.start_features(x)\n",
        "        deep_features = None\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            out = block(out)\n",
        "            if i == 5:\n",
        "                deep_features = out\n",
        "\n",
        "        return out, deep_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl1Z4azt3ubf"
      },
      "source": [
        "def densenet(pretrained=True):\n",
        "    return DenseNet(pretrained=pretrained)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AzVNny23xi_"
      },
      "source": [
        "class PSPModule(nn.Module):\n",
        "    def __init__(self, features, out_features=1024, sizes=(1, 2, 3, 6)):\n",
        "        super().__init__()\n",
        "        self.stages = []\n",
        "        self.stages = nn.ModuleList([self._make_stage(features, size) for size in sizes])\n",
        "        self.bottleneck = nn.Conv2d(features * (len(sizes) + 1), out_features, kernel_size=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def _make_stage(self, features, size):\n",
        "        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\n",
        "        conv = nn.Conv2d(features, features, kernel_size=1, bias=False)\n",
        "        return nn.Sequential(prior, conv)\n",
        "\n",
        "    def forward(self, feats):\n",
        "        h, w = feats.size(2), feats.size(3)\n",
        "        priors = [F.interpolate(input=stage(feats), size=(h, w), mode='bilinear', align_corners=False) for stage in self.stages] + [feats]\n",
        "        bottle = self.bottleneck(torch.cat(priors, 1))\n",
        "        return self.relu(bottle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0ohH2iB39AY"
      },
      "source": [
        "class PSPUpsample(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = 2 * x.size(2), 2 * x.size(3)\n",
        "        p = F.interpolate(input=x, size=(h, w), mode='bilinear', align_corners=False)\n",
        "        return self.conv(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR4I8YTp4AMF"
      },
      "source": [
        "class PSPNet(nn.Module):\n",
        "    def __init__(self, n_classes=20, sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet34',\n",
        "                 pretrained=True):\n",
        "        super().__init__()\n",
        "        self.feats = densenet(pretrained)\n",
        "        self.psp = PSPModule(psp_size, 1024, sizes)\n",
        "        self.drop_1 = nn.Dropout2d(p=0.3)\n",
        "\n",
        "        self.up_1 = PSPUpsample(1024, 256)\n",
        "        self.up_2 = PSPUpsample(256, 64)\n",
        "        self.up_3 = PSPUpsample(64, 64)\n",
        "\n",
        "        self.drop_2 = nn.Dropout2d(p=0.15)\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Conv2d(64, n_classes, kernel_size=1),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(deep_features_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f, class_f = self.feats(x) \n",
        "        p = self.psp(f)\n",
        "        p = self.drop_1(p)\n",
        "\n",
        "        p = self.up_1(p)\n",
        "        p = self.drop_2(p)\n",
        "\n",
        "        p = self.up_2(p)\n",
        "        p = self.drop_2(p)\n",
        "\n",
        "        p = self.up_3(p)\n",
        "        p = self.drop_2(p)\n",
        "\n",
        "        auxiliary = F.adaptive_max_pool2d(input=class_f, output_size=(1, 1)).view(-1, class_f.size(1))\n",
        "\n",
        "        return self.final(p), self.classifier(auxiliary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJyYN9qA4IS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b39ee6c-19a3-4688-c165-3bd3e81f2c83"
      },
      "source": [
        "models = {\n",
        "    'densenet': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=1024, deep_features_size=512, backend='densenet')\n",
        "}\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "if torch.cuda.is_available():\n",
        "    map_location=lambda storage, loc: storage.cuda()\n",
        "else:\n",
        "    map_location='cpu'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaEkK1gQP2bg"
      },
      "source": [
        "def build_network(snapshot, backend):\n",
        "    epoch = 0\n",
        "    backend = backend.lower()\n",
        "    net = models[backend]()\n",
        "    net = nn.DataParallel(net)\n",
        "    if snapshot is not None:\n",
        "        _, epoch = os.path.basename(snapshot).split('_')\n",
        "        if not epoch == 'last':\n",
        "            epoch = int(epoch)\n",
        "        net.load_state_dict(torch.load(snapshot, map_location=map_location))\n",
        "        logging.info(\"Snapshot for epoch {} loaded from {}\".format(epoch, snapshot))\n",
        "    net = net.to(device)\n",
        "    return net, epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54rmhdBb9N1w"
      },
      "source": [
        "def get_transform():\n",
        "    transform_image_list = [\n",
        "        transforms.Resize((256, 256), 3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]\n",
        "    return transforms.Compose(transform_image_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz3WORpYHyBO"
      },
      "source": [
        "class saveThread (threading.Thread):\n",
        "   def __init__(self, dirctry, fileName, port):\n",
        "      threading.Thread.__init__(self)\n",
        "      self.dirctry = dirctry\n",
        "      self.fileName = fileName\n",
        "      self.port = port\n",
        "   def run(self):\n",
        "      save(self.dirctry, self.fileName, self.port)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gg_fqo39QtD"
      },
      "source": [
        "def save(dirctry, fileName, port):\n",
        "    path = dirctry + '/'+fileName\n",
        "    print(path)\n",
        "    print(fileName)\n",
        "    service = BlobServiceClient.from_connection_string(conn_str=connection_string)\n",
        "    blob_client = service.get_blob_client(container='imagegen', blob='BodyMask/'+fileName+'.jpg')\n",
        "    try:\n",
        "        with open(fileName+'bodymask.jpg', 'rb') as data:  \n",
        "            blob_client.upload_blob(data,overwrite=True)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "    blob_client = service.get_blob_client(container='imagegen', blob='ClothMask/'+fileName+'.jpg')\n",
        "    try:\n",
        "        with open(fileName+'clothmask.jpg', 'rb') as data:  \n",
        "            blob_client.upload_blob(data,overwrite=True)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "    blob_client = service.get_blob_client(container='imagegen', blob='HeadMask/'+fileName+'.jpg')\n",
        "    try:\n",
        "        with open(fileName+'headmask.jpg', 'rb') as data:  \n",
        "            blob_client.upload_blob(data,overwrite=True)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "    blob_client = service.get_blob_client(container='imagegen', blob='Head/'+fileName+'.jpg')\n",
        "    try:\n",
        "        with open(fileName+'head.jpg', 'rb') as data:  \n",
        "            blob_client.upload_blob(data,overwrite=True)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "    blob_client = service.get_blob_client(container='imagegen', blob='Cloth/'+fileName+'.jpg')\n",
        "    try:\n",
        "        with open(fileName+'cloth.jpg', 'rb') as data:  \n",
        "            blob_client.upload_blob(data,overwrite=True)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return\n",
        "    print('uploaded')\n",
        "    os.remove(fileName+'cloth.jpg')\n",
        "    os.remove(fileName+'headmask.jpg')\n",
        "    os.remove(fileName+'clothmask.jpg')\n",
        "    os.remove(fileName+'bodymask.jpg')\n",
        "    os.remove(fileName+'head.jpg')\n",
        "    os.remove(fileName+'temp.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP0K2-5C9aSL"
      },
      "source": [
        "def show_image(img, pred, temp_image, image_url):\n",
        "    # fig, axes = plt.subplots(1, 2)\n",
        "    # ax0, ax1 = axes\n",
        "    # ax0.get_xaxis().set_ticks([])\n",
        "    # ax0.get_yaxis().set_ticks([])\n",
        "    # ax1.get_xaxis().set_ticks([])\n",
        "    # ax1.get_yaxis().set_ticks([])\n",
        "\n",
        "    classes = np.array(('Background',  # always index 0\n",
        "                        'Hat', 'Hair', 'Glove', 'Sunglasses',\n",
        "                        'UpperClothes', 'Dress', 'Coat', 'Socks',\n",
        "                        'Pants', 'Jumpsuits', 'Scarf', 'Skirt', \n",
        "                        'Face', 'Left-arm', 'Right-arm', 'Left-leg',\n",
        "                        'Right-leg', 'Left-shoe', 'Right-shoe'))\n",
        "    colormap = [(0, 0, 0),\n",
        "                (1, 0.25, 0), (0, 0.25, 0), (0.5, 0, 0.25), (1, 1, 1),\n",
        "                (1, 0.75, 0), (0, 0, 0.5), (0.5, 0.25, 0), (0.75, 0, 0.25),\n",
        "                (1, 0, 0.25), (0, 0.5, 0), (0.5, 0.5, 0), (0.25, 0, 0.5),\n",
        "                (1, 0, 0.75), (0, 0.5, 0.5), (0.25, 0.5, 0.5), (1, 0, 0),\n",
        "                (1, 0.25, 0), (0, 0.75, 0), (0.5, 0.75, 0), ]\n",
        "    cmap = matplotlib.colors.ListedColormap(colormap)\n",
        "    bounds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
        "    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "    h, w, _ = pred.shape\n",
        "\n",
        "    def denormalize(img, mean, std):\n",
        "        c, _, _ = img.shape\n",
        "        for idx in range(c):\n",
        "            img[idx, :, :] = img[idx, :, :] * std[idx] + mean[idx]\n",
        "        return img\n",
        "\n",
        "    img = denormalize(img.cpu().numpy(), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    img = img.transpose(1, 2, 0).reshape((h, w, 3))\n",
        "    pred = pred.reshape((h, w))\n",
        "    print(pred.shape)\n",
        "    \n",
        "    masks = []\n",
        "    for i in range(len(classes)):\n",
        "      masks.append((pred == i))\n",
        "\n",
        "    clothmask = (pred==5.0) | (pred==15.0)| (pred==14.0)\n",
        "    #headmask = (pred == 1.0) | (pred == 2.0) | (pred == 4.0) | (pred == 13.0)\n",
        "    dress = (pred == 6.0)\n",
        "    pants=(pred==9.0)\n",
        "    jumpsuits=(pred==10.0)\n",
        "    Skirt=(pred==12.0)   \n",
        "\n",
        "    # bodymask = (pred != 2.0) & (pred != 1.0) & (pred != 4.0) & (pred != 13.0) & (pred != 0.0)\n",
        "\n",
        "    orig_image = Image.open(temp_image)\n",
        "    sec_image = Image.new(\"RGB\", orig_image.size, (255,255,255))\n",
        "    \n",
        "    clothmask = Image.fromarray(clothmask).resize(orig_image.size)\n",
        "    print(orig_image.size)\n",
        "    print(clothmask.size)\n",
        "    clothmask.save(\"clothmask.jpg\")\n",
        "    cloth = Image.composite(orig_image, sec_image, clothmask)\n",
        "    cloth.save('/content/drive/My Drive/Myntra_hackerRamp/cloth.jpg') \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4C2H7Rn9bTn"
      },
      "source": [
        "def main(image_url):\n",
        "    backend='densenet'\n",
        "    models_path='/content/drive/My Drive/Myntra_hackerRamp'\n",
        "    # csv_file='images_url_temp.csv'\n",
        "    # --------------- model --------------- #\n",
        "    snapshot = os.path.join(models_path, backend, 'PSPNet_last')\n",
        "    net, starting_epoch = build_network(snapshot, backend)\n",
        "    net.eval()\n",
        "\n",
        "    # ------------ load image ------------ #\n",
        "    # df = pd.read_csv(csv_file)\n",
        "    # images = df['image_url'].to_numpy()\n",
        "    data_transform = get_transform()\n",
        "    tic_whole = time.time()\n",
        "    print('start processing...')\n",
        "    saving_thread=0\n",
        "    print(image_url)\n",
        "    temp_image = 'image.jpg'\n",
        "    urllib.request.urlretrieve(str(image_url), temp_image)\n",
        "    img = Image.open(temp_image,'r')\n",
        "    img = data_transform(img)\n",
        "    img = img.to(device)\n",
        "    with torch.no_grad():\n",
        "        tic = time.time()\n",
        "        pred, _ = net(img.unsqueeze(dim=0))\n",
        "        toc = time.time()\n",
        "        print('Prediction in %.5f seconds' % (toc - tic))\n",
        "        pred = pred.squeeze(dim=0)\n",
        "        pred = pred.cpu().numpy().transpose(1, 2, 0)\n",
        "        pred = np.asarray(np.argmax(pred, axis=2), dtype=np.uint8).reshape((256, 256, 1))\n",
        "        show_image(img, pred, temp_image, image_url)\n",
        "    toc_whole = time.time()\n",
        "    print ('Time Elapsed is %.5f' % (toc_whole - tic_whole))\n",
        "        \n",
        "        \n",
        "    # generate image with body parts\n",
        "    # saving_thread.join()\n",
        "    print ('processing time is %.5f' % (toc_whole - tic_whole))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64tATJASXJHK",
        "outputId": "1ab1b3be-4328-4ab9-8d6b-cb82e54ef762"
      },
      "source": [
        "main('https://i.ibb.co/7SZnVkz/quiero.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:281: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start processing...\n",
            "https://i.ibb.co/7SZnVkz/quiero.jpg\n",
            "Prediction in 1.26409 seconds\n",
            "(256, 256)\n",
            "(526, 700)\n",
            "(526, 700)\n",
            "Time Elapsed is 2.85353\n",
            "processing time is 2.85353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18JbVk5vrf0U",
        "outputId": "8d93f353-7a6c-42a0-f88b-f7729cf25c19"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    }
  ]
}